# Solving the Two Player Soccer Game with Multi-agent Q-learnings
## What is Soccer Game
A special form of soccer game is the problem to solve as well as the environment to work in. It features 2 players over a game grid of 2(rows)*4(columns) cells. The ball may be possessed by either of the players. So the total number of states is 8(possible cells for player A) *7(possible cells for player B, since B cannot occupy the same cell as player A)*2(possession of the ball) = 112. The size of action space is 5(possible actions for player A)*5(possible actions for player B), yielding a total of 25 action combinations. The five actions N, S, E, W & Stick. The first four actions indicate the directions of the action, whereas 'stick' means not to move. Each moving action will only help the player move one cell, unless the action ought to push the player out of the grid, in which case the player will stick. <br />
In terms of reward, the two cells on the far left are goals for player A and the two cells on the far right are goals for player B. A player will be rewarded +100 if they bring the ball into their own goal but -100 if they bring the ball into their opponent's goal.The game is zero-sum, meaning the opponent's reward is the opposite of the reward earned by the player who enter the goal cells with a ball. For all other transitions, the reward is 0. <br />
It is worth noting that our environment has no deterministic equilibrium policies, because for example in the state pictured below, B could be blocked by A regardless of the action B chooses. Therefore, it is important to find an algorithm that can converge to non-deterministic policies, namely foe-q and corr-q. Non-deterministic means given the states, the algorithm can predict more than one actions for the player to choose from. <br />
The soccer environment is coded in **soccer_env.py**
## Multi-agent Q-learning algorithms
4 multi agent Q learning algorithms are implemented: 
Q learning only takes into consideration player A's action values, so we only need one Q table to represent them. It has 4 dimensions: 4 for the states and 1 for actions of A. <br />
Different from Q-learning, friend-Q adds in another player, and assumes that player B acts in the best interest of Player A. Every action player B takes is to maximize player A's value accumulation. Therefore, we can imagine player B to be a friend. We still need only one Q table, but including another dimension representing player B's actions. <br />
In Foe-Q learning, we assume Player B to completely hostile and will take whichever action that can minimize player A's value. The Q table is set up in a same way as Friend-Q, with 5 dimensions. The method of getting the objective V is different. In this fully competitive setting, player B will try to minimize over player A's actions before maximizing over his own actions. Player B learns how to defeat A based on how A acts and update its action values accordingly.<br />
uCE-Q stands for utilitarian correlated equilibrium learning. Correlated Equilibria is a probability distribution over the joint space of actions (Greenwald and Hall, 2003).It takes into consideration that players actions can be dependent on each other.
* **q_learning.ipynb**: run all the cells in this notebook will train a q_learning agent and output the error chart. 
* **friend_q.ipynb**: run all the cells in this notebook will train a friend_q agent and output the error chart. 
* **foe_q.ipynb**: run all the cells in this notebook will train a foe_q agent and output the error chart.
* **uCE_Q.ipynb**: run all the cells in this notebook will train a uCE_Q agent and output the error chart.
* **ceQ-chicken.ipynb**: this notebook implements the same uCE value update as in uCE_Q.ipynb to find the objective function, and is applied to the Chicken-Dare game.

